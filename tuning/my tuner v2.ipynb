{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46fbcf17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:39:29.009940: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-30 10:39:34.920089: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/albert/miniconda3/envs/Baikal2/lib/:/home/albert/miniconda3/envs/Baikal2/lib/\n",
      "2023-05-30 10:39:34.920296: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/albert/miniconda3/envs/Baikal2/lib/:/home/albert/miniconda3/envs/Baikal2/lib/\n",
      "2023-05-30 10:39:34.920314: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "# GPU on\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    print(gpu)\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "import tensorflow.keras.layers as tfl\n",
    "import keras_tuner as kt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../nn_builds')\n",
    "\n",
    "from nn_constructor import *\n",
    "from nn_to_tune import *\n",
    "from ds_making import *\n",
    "import losses\n",
    "\n",
    "from compare_auc_delong_xu import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b5760ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_preds(preds, labels):\n",
    "    with h5.File(path_to_h5, 'r') as hf:\n",
    "        idxs_mu = np.where(labels == 0)[0]\n",
    "        idxs_nu = np.where(labels == 1)[0]\n",
    "        preds_mu = preds[idxs_mu]\n",
    "        preds_nu = preds[idxs_nu]\n",
    "    return preds_mu, preds_nu\n",
    "\n",
    "def plot_hists(preds, labels, bins = 100, regime = 'test'):  \n",
    "    preds_mu, preds_nu = separate_preds(preds, labels)\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    plt.title(\"Density of events vs predictions\", fontsize=14)\n",
    "    plt.hist(preds_mu, bins = bins, histtype = 'step', density = True, log = True, label = 'only Mu')\n",
    "    plt.hist(preds_nu, bins = bins, histtype = 'step', density = True, log = True, label = 'only Nu')\n",
    "    plt.legend(fontsize=14, loc=9)\n",
    "    plt.xlabel(\"predicted confidence\", fontsize=14)\n",
    "    plt.ylabel(\"density of events\", fontsize=14)\n",
    "    #plt.savefig('Figures/'+model_name+'/Hist_of_events_'+regime+'.png')\n",
    "    #plt.close(fig)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5994dc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expos(tr, preds_nu):\n",
    "    num = np.sum(np.where(preds_nu>tr, 1, 0))\n",
    "    return num/len(preds_nu)\n",
    "\n",
    "def array_expos(preds_nu, arr_tr = np.linspace(0,1,1000)):\n",
    "    arr = np.zeros(len(arr_tr))\n",
    "    for n, tr in enumerate(arr_tr):\n",
    "        arr[n] = expos(tr,preds_nu)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "838abefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_metric(labels,preds):\n",
    "    preds_mu, preds_nu = separate_preds(preds, labels)\n",
    "    index_mu = np.where(array_expos(preds_mu) < 5e-5)[0][0]\n",
    "    tr_best = index_mu/10**int(np.log10(index_mu)+1)\n",
    "    return expos(tr_best, preds_nu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a4830ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_to_tune(model, path_to_h5, batch_size, lr_initial, \n",
    "                model_name,\n",
    "                num_of_epochs = 10, steps_per_epoch = 1e4, verbose = 1, is_mask = True):\n",
    "    with h5.File(path_to_h5, 'r') as hf:\n",
    "        total_num = hf['train/data/data'].shape[0]\n",
    "        total_steps = total_num//batch_size\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = lr_initial, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False, name='Adam')\n",
    "    model.compile(optimizer=optimizer, loss=losses.focal_loss(2., 2., 10., 1.))\n",
    "    callbacks = [tf.keras.callbacks.EarlyStopping(monitor='loss', patience=2, min_delta = 2e-3)]\n",
    "\n",
    "    train_dataset = make_train_dataset(path_to_h5, batch_size, Shape, is_mask = is_mask)\n",
    "\n",
    "    history = model.fit(train_dataset, \n",
    "                        steps_per_epoch=steps_per_epoch, epochs=num_of_epochs, \n",
    "                        verbose=verbose, callbacks=callbacks)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afdcb07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f3452a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"baikal_mu-nu_h5-s2_tres08_old_norm.h5\"\n",
    "path_to_h5 = '../../../../../../ivkhar/Baikal/data/' + file_name\n",
    "mn = 'nn_mask_1lstm_3resblocks_1lstm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1666205f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory for the model already exists\n"
     ]
    }
   ],
   "source": [
    "model_name = mn + '_noise_' + file_name[0:-3]\n",
    "is_mask = True\n",
    "try:\n",
    "    os.makedirs('logs/'+model_name)\n",
    "    print('directory for the model is created')\n",
    "except:\n",
    "    print('directory for the model already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c7ee7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the shape of data\n",
    "with h5.File(path_to_h5, 'r') as hf:\n",
    "    Shape = hf['train/data/data'].shape[1:]\n",
    "    if is_mask:\n",
    "        Shape = tuple([None,Shape[1]+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2414e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#space of hyperparams\n",
    "\n",
    "k_id_list = [0]*3\n",
    "f_id_list = [0]*3\n",
    "k_cd_list = [0]*3\n",
    "f_cd_list = [0]*3\n",
    "s_cd_list = [0]*3\n",
    "\n",
    "u1_list = [4,8,16]\n",
    "\n",
    "k_id_list[0] = [4,8,16]\n",
    "k_id_list[1] = [4,8,16]\n",
    "k_id_list[2] = [4,8,16]\n",
    "\n",
    "k_cd_list = k_id_list\n",
    "\n",
    "f_id_list[0] = [64] #[32,64,128]\n",
    "f_id_list[1] = [128] #[32,64,128]\n",
    "f_id_list[2] = [64] #[32,64,128]\n",
    "\n",
    "f_cd_list = f_id_list\n",
    "\n",
    "s_cd_list = [[2],[2],[2]]\n",
    "\n",
    "u2_list = [4,8,16]\n",
    "\n",
    "lr_list = [0.005]\n",
    "batch_size = 256\n",
    "\n",
    "keys = ['u1','u2',\n",
    "        'f_id_list_0','f_id_list_1','f_id_list_2',\n",
    "        'k_id_list_0','k_id_list_1','k_id_list_2',\n",
    "        'f_cd_list_0','f_cd_list_1','f_cd_list_2',\n",
    "        'k_cd_list_0','k_cd_list_1','k_cd_list_2',\n",
    "        's_cd_list_0','s_cd_list_1','s_cd_list_2']\n",
    "space = {'u1':u1_list,'u2':u2_list,\n",
    "        'f_id_list_0':f_id_list[0],'f_id_list_1':f_id_list[1],'f_id_list_2':f_id_list[2],\n",
    "        'k_id_list_0':k_id_list[0],'k_id_list_1':k_id_list[1],'k_id_list_2':k_id_list[2],\n",
    "        'f_cd_list_0':f_cd_list[0],'f_cd_list_1':f_cd_list[1],'f_cd_list_2':f_cd_list[2],\n",
    "        'k_cd_list_0':k_cd_list[0],'k_cd_list_1':k_cd_list[1],'k_cd_list_2':k_cd_list[2],\n",
    "        's_cd_list_0':s_cd_list[0],'s_cd_list_1':s_cd_list[1],'s_cd_list_2':s_cd_list[2]}\n",
    "\n",
    "def generate_idxs(space = space):\n",
    "    keys = space.keys()\n",
    "    lengths = [len(space[k]) for k in keys]\n",
    "    arrays = [np.arange(l) for l in lengths]\n",
    "    t = np.meshgrid(*arrays)\n",
    "    for i in range(len(t)):\n",
    "        t[i] = t[i].flatten()\n",
    "    for i in zip(*t):\n",
    "        idxs = list(i)\n",
    "        yield idxs \n",
    "\n",
    "def make_config(idxs, space = space):\n",
    "    keys = list(space.keys())\n",
    "    assert len(idxs) == len(keys)\n",
    "    config = dict.fromkeys(keys)\n",
    "    for i,k in enumerate(keys):\n",
    "        config[k] = space[k][idxs[i]]\n",
    "    f_id_list = [config[keys[m]] for m in range(2,5)]\n",
    "    #k_id_list = [config[keys[m]] for m in range(5,8)]\n",
    "    f_id_list = [config[keys[m]] for m in range(8,11)]\n",
    "    k_cd_list = [config[keys[m]] for m in range(11,14)]\n",
    "    k_id_list = k_cd_list #temporary\n",
    "    s_cd_list = [config[keys[m]] for m in range(14,17)]\n",
    "    keys_new = ['u1','u2',\n",
    "                'f_id_list', \n",
    "                'k_id_list',\n",
    "                'f_cd_list',\n",
    "                'k_cd_list',\n",
    "                's_cd_list']\n",
    "    config_new = {'u1':config['u1'],'u2':config['u2'],\n",
    "                'f_id_list':f_id_list, \n",
    "                'k_id_list':k_id_list,\n",
    "                'f_cd_list':f_id_list,\n",
    "                'k_cd_list':k_cd_list,\n",
    "                's_cd_list':s_cd_list}\n",
    "    return config_new "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b49d0bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = make_test_dataset(path_to_h5, batch_size, (None,None,6), is_mask = is_mask)\n",
    "with h5.File(path_to_h5, 'r') as hf:\n",
    "    labels = np.zeros((339968, 2))\n",
    "    ids = hf['test/ev_ids/data'][:]  # id of event - starting with 'nu' or 'mu'\n",
    "    ids = np.array([i[0] for i in ids]).reshape(ids.shape[0],1)\n",
    "    labels = np.where(ids[0:339968] == 110, [0,1], [1,0])  # 110 - byte code for 'n'\n",
    "labels = labels[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc608c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max steps in trial: 20000\n",
      "Trial # is  1\n",
      "Learning rate: 0.005 .\n",
      "Config: {'u1': 4, 'u2': 4, 'f_id_list': [64, 128, 64], 'k_id_list': [4, 4, 4], 'f_cd_list': [64, 128, 64], 'k_cd_list': [4, 4, 4], 's_cd_list': [2, 2, 2]}\n",
      "Epoch 1/20\n",
      "1000/1000 [==============================] - 78s 58ms/step - loss: 0.0436\n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 59s 59ms/step - loss: 0.0149\n",
      "Epoch 3/20\n",
      " 899/1000 [=========================>....] - ETA: 6s - loss: 0.0123"
     ]
    }
   ],
   "source": [
    "lr = 5e-3\n",
    "f_list = [64,128,64]\n",
    "\n",
    "\n",
    "steps_per_epoch = 1e3\n",
    "num_of_epochs = 20\n",
    "print('Max steps in trial:', int(steps_per_epoch*num_of_epochs))\n",
    "\n",
    "trial = 0\n",
    "logs = []\n",
    "config_list = []\n",
    "auc_list, metric_list = [], []\n",
    "\n",
    "I = generate_idxs()\n",
    "for j in range(27):\n",
    "    config = make_config(next(I))\n",
    "    trial +=1\n",
    "    print('Trial # is ', trial)\n",
    "    metrics = []\n",
    "    aucs = []\n",
    "    print('Learning rate:', lr,'.\\nConfig:', config)\n",
    "    for i in range(4):\n",
    "        model = globals()[mn]((None,6), **config)\n",
    "        history = train_model_to_tune(model, path_to_h5, batch_size, lr, \n",
    "                                      model_name, steps_per_epoch = steps_per_epoch, num_of_epochs = num_of_epochs)\n",
    "        print('Predictions on test dataset:')\n",
    "        preds = model.predict(test_dataset)\n",
    "        preds = preds[:,1]\n",
    "        metric = my_metric(labels,preds)\n",
    "        auc, std_auc = delong_roc_variance(labels,preds)\n",
    "        metrics.append(metric)\n",
    "        aucs.append([auc,std_auc])\n",
    "        print('AUC:', str(np.round(auc, 6))+'+-'+str(np.round(std_auc, 14)),'. Exposition:', np.round(metric,4), '. \\n')\n",
    "    config_list.append(config)\n",
    "    auc_list.append(np.array(aucs)), metric_list.append(np.array(metrics))\n",
    "res = {'auc':auc_list, 'my_metric': metric_list}\n",
    "logs.append([config_list, res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55fe74ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'u1': 4, 'u2': 4, 'f_id_list': [64, 128, 64], 'k_id_list': [4, 8, 8], 'f_cd_list': [64, 128, 64], 'k_cd_list': [4, 8, 8], 's_cd_list': [2, 2, 2]}\n"
     ]
    }
   ],
   "source": [
    "next(I)\n",
    "config = make_config(next(I))\n",
    "#config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8bbf6616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'u1': 4,\n",
       " 'u2': 4,\n",
       " 'f_id_list': [64, 128, 64],\n",
       " 'k_id_list': [16, 16, 16],\n",
       " 'f_cd_list': [64, 128, 64],\n",
       " 'k_cd_list': [16, 16, 16],\n",
       " 's_cd_list': [2, 2, 2]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "193c2d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trial_info(i, logs = logs):\n",
    "    info = dict.fromkeys(logs[0][0][0].keys())\n",
    "    for k in logs[0][0][0].keys():\n",
    "        info[k] = logs[0][0][i][k]\n",
    "    for k in logs[0][1].keys():\n",
    "        info[k] = logs[0][1][k][i]\n",
    "    info['auc'] = min([j[0] for j in info['auc']])\n",
    "    info['my_metric_min'] = min([j for j in info['my_metric']])\n",
    "    info['my_metric_max'] = max([j for j in info['my_metric']])\n",
    "    info['my_metric_mean'] = np.mean([j for j in info['my_metric']])\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7d783011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'u1': 4,\n",
       " 'u2': 4,\n",
       " 'f_id_list': [64, 128, 64],\n",
       " 'k_id_list': [4, 4, 8],\n",
       " 'f_cd_list': [64, 128, 64],\n",
       " 'k_cd_list': [4, 4, 8],\n",
       " 's_cd_list': [2, 2, 2],\n",
       " 'auc': 0.999749776849993,\n",
       " 'my_metric': array([0.92109144, 0.91459788, 0.91880199, 0.92195507]),\n",
       " 'my_metric_min': 0.91459788001988,\n",
       " 'my_metric_max': 0.9219550746718593,\n",
       " 'my_metric_mean': 0.919111596340142}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm = 0\n",
    "for n, l in enumerate(logs[0][0]):\n",
    "    info = get_trial_info(n)\n",
    "    i = info['my_metric_mean']\n",
    "    if i>mm:\n",
    "        mm = i\n",
    "        t = n\n",
    "print('Best trial is:')\n",
    "get_trial_info(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "18086c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'u1': 4,\n",
       " 'u2': 4,\n",
       " 'f_id_list': [64, 128, 64],\n",
       " 'k_id_list': [4, 8, 8],\n",
       " 'f_cd_list': [64, 128, 64],\n",
       " 'k_cd_list': [4, 8, 8],\n",
       " 's_cd_list': [2, 2, 2],\n",
       " 'auc': 0.9998100738097733,\n",
       " 'my_metric': array([0.88609792, 0.91376683, 0.91588519, 0.86954219]),\n",
       " 'my_metric_min': 0.8695421918410912,\n",
       " 'my_metric_max': 0.9158851853964167,\n",
       " 'my_metric_mean': 0.8963230321744869}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_trial_info(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddef23bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
