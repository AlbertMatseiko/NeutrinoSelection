{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33dbb3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py as h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3033f336",
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_name = 'baikal_multi_0523_flat_pureMC_h5s2_norm.h5'\n",
    "path_to_data = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1fa7ad66",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_h5 = path_to_data + h5_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "9accb915",
   "metadata": {},
   "outputs": [],
   "source": [
    "class info_Baikal_HDF5:\n",
    "    def __init__(self, h5_name, path_to_data = './data/'):\n",
    "        self.path = path_to_data + h5_name\n",
    "        #self.nums_in_train, self.nums_in_test, self.nums_in_val = None, None, None\n",
    "        self.nums_dict = {'train': None, 'test': None, 'val': None}\n",
    "        self.masks_dict = {'train': None, 'test': None, 'val': None}\n",
    "\n",
    "    def collect_info(self, regime):\n",
    "        with h5.File(self.path, 'r') as hf:\n",
    "            ids = hf[regime+'/ev_ids_corr/data']\n",
    "            print('ids_collected')\n",
    "            l_nuatm, l_nue2, l_mu = [], [], []\n",
    "            for i in ids:\n",
    "                l_nuatm.append(i.startswith(b'nuatm'))\n",
    "                l_nue2.append(i.startswith(b'nue2'))\n",
    "                l_mu.append(i.startswith(b'mu'))\n",
    "            num_mu = sum(l_mu)\n",
    "            print('mu_collected')\n",
    "            num_atm_nu = sum(l_nuatm)\n",
    "            print('nu_atm_collected')\n",
    "            num_e2_nu = sum(l_nue2)\n",
    "            print('nu_e2_collected')\n",
    "            self.nums_dict[regime] = {'mu': num_mu, 'nu_atm': num_atm_nu, 'nu_e2': num_e2_nu}  \n",
    "            self.masks_dict[regime] = {'mu': l_mu, 'nu_atm': l_nuatm, 'nu_e2': l_nue2}\n",
    "\n",
    "    def get_info_about(self, regime, return_masks = False):\n",
    "        if not return_masks:\n",
    "            return self.nums_dict[regime]\n",
    "        else:\n",
    "            return self.nums_dict[regime], self.masks_dict[regime]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da4c913",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparation to make small h5, ~30 min\n",
    "info = info_Baikal_HDF5(h5_name)\n",
    "for r in ['train','test','val']:\n",
    "    print(r)\n",
    "    info.collect_info(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "a5fb7547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555614\n",
      "46525\n",
      "2234582\n"
     ]
    }
   ],
   "source": [
    "# makes small h5 from bigger one\n",
    "path_to_h5_small = path_to_data + h5_name[0:-3] + '_small.h5'\n",
    "\n",
    "with h5.File(path_to_h5_small, 'w') as fs:\n",
    "    with h5.File(path_to_h5, 'r') as hf:\n",
    "        fs.create_dataset('norm_param/mean', data = hf['norm_param/mean'])\n",
    "        fs.create_dataset('norm_param/std', data = hf['norm_param/std'])\n",
    "        for r in ['train','test','val']:\n",
    "            #set size of new file to 1/10 of initial one\n",
    "            new_length = hf[r+'/ev_ids_corr/data'].shape[0]//10\n",
    "            print(new_length)\n",
    "            if r == 'train':\n",
    "                start = 0\n",
    "                stop = start+new_length\n",
    "                start_data = hf[r+'/ev_starts/data'][start]\n",
    "                stop_data = hf[r+'/ev_starts/data'][stop]\n",
    "                \n",
    "                ev_starts = hf[r+'/ev_starts/data'][start : stop]\n",
    "                ids = hf[r+'/ev_ids_corr/data'][start : stop]\n",
    "                data = hf[r+'/data/data'][start_data:stop_data]\n",
    "                \n",
    "                fs.create_dataset(r+'/ev_starts/data', data = ev_starts)\n",
    "                fs.create_dataset(r+'/data/data', data = data)\n",
    "                fs.create_dataset(r+'/ev_ids_corr/data', data = ids)\n",
    "                \n",
    "            else:\n",
    "                start1 = 0\n",
    "                stop1 = start1+new_length//2\n",
    "                start_data1 = hf[r+'/ev_starts/data'][start1]\n",
    "                stop_data1 = hf[r+'/ev_starts/data'][stop1]\n",
    "                \n",
    "                #finding where muons end\n",
    "                start2 = info.nums_dict[r]['mu']\n",
    "                stop2 = start2+new_length//4\n",
    "                start_data2 = hf[r+'/ev_starts/data'][start2]\n",
    "                stop_data2 = hf[r+'/ev_starts/data'][stop2]\n",
    "                \n",
    "                #finding where atmosheric neutrinos end\n",
    "                start3 = info.nums_dict[r]['mu']+info.nums_dict[r]['nu_atm']\n",
    "                stop3 = start3+new_length//4\n",
    "                start_data3 = hf[r+'/ev_starts/data'][start3]\n",
    "                stop_data3 = hf[r+'/ev_starts/data'][stop3]\n",
    "                \n",
    "                ev_starts = np.concatenate([hf[r+'/ev_starts/data'][start1:stop1],\n",
    "                                            hf[r+'/ev_starts/data'][start2:stop2],\n",
    "                                            hf[r+'/ev_starts/data'][start3:stop3]],\n",
    "                                           axis = 0)\n",
    "\n",
    "                ids = np.concatenate([hf[r+'/ev_ids_corr/data'][start1:stop1],\n",
    "                                      hf[r+'/ev_ids_corr/data'][start2:stop2],\n",
    "                                      hf[r+'/ev_ids_corr/data'][start3:stop3]],\n",
    "                                     axis = 0)\n",
    "                \n",
    "                data = np.concatenate([hf[r+'/data/data'][start_data1:stop_data1],\n",
    "                                      hf[r+'/data/data'][start_data2:stop_data2],\n",
    "                                      hf[r+'/data/data'][start_data3:stop_data3]],\n",
    "                                     axis = 0)\n",
    "                \n",
    "                fs.create_dataset(r+'/ev_starts/data', data = ev_starts)\n",
    "                fs.create_dataset(r+'/data/data', data = data)\n",
    "                fs.create_dataset(r+'/ev_ids_corr/data', data = ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
